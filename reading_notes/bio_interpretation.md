* SHAP value
* weights Way and Green
* latent arithmetic Way + see the other ref
* network architecture
* enrichment

Shu et al. 2021 DeepSEM, a deep
generative model that can jointly embed the gene expression data and simultaneously construct a GRN that reflects the inner struc- ture of gene interactions in single cells without relying on any addi- tional information such as TF binding motifs or single-cell ATAC sequencing (scATAC-seq) data.

a beta-variational autoencoder (beta-VAE)23
in which
the weights of both the encoder and decoder functions represent the adjacency matrix of the GRN.



By inspecting the architecture of the model that represents the inner workings of a cell, we can observe how multiple genes interact with each other to determine the expression levels of individual genes

Another important functional component of DeepSEM is to
simulate scRNA-seq data by perturbing the values of its hidden neu- rons. In silico data simulations have already achieved tremendous success in computer vision for data augmentation, especially when the number of training samples is limited24
. In single-cell biology,
the same types of simulation algorithm have also been applied to scRNA-seq data to predict the single-cell perturbation response out of sample19,25
, identify marker genes26
and augment the sparse cell
populations to improve the accuracy of cell-type classification27



Different from conventional deep learning models that embed the expressions of all the genes together into a latent space6,7
,
the encoder function of DeepSEM takes the expression of only one gene as the input feature of the neural network. The neural net- works for different genes share their weights or it could be viewed as using one neural network to scan all the genes. At this step, there are no interactions among different genes in the model. Later, another two fully connected neural networks transform the output of these small neural networks to the posterior mean and standard devia- tion of a multivariate Gaussian distribution. Decoupling the non- linear operation and the gene interaction is the key for DeepSEM to achieve more robust and interpretable hidden representations at the same time.



Structural equation modeling is a multivariate statistical model to analyze structural relationships among different random variables



the SEM can be adopted to detect the conditional dependency among random variables and therefore also used to predict the graph structure of Bayesian networks and Markov random fields53–56
. DeepSEM generalizes the SEM, which
models the conditional dependencies among random variables and is formulated as a self-regression problem





Cao and Gao 2021

Taking advantage of prior biological knowledge, we propose the use of a knowledge-based graph 2 (“guidance graph”) that explicitly models cross-layer regulatory interactions for linking layer- 3 specific feature spaces; the vertices in the graph correspond to the features of different omics layers, 4 and edges represent signed regulatory interactions.

Combining omics-specific autoencoders with graph-based coupling and adversarial alignment, we 24 designed and implemented the GLUE framework for unpaired single-cell multi-omics data 25 integration with superior accuracy and robustness. By modeling regulatory interactions across omics 26 layers explicitly, GLUE uniquely supports model-based regulatory inference for unpaired multi- 27 omics datasets, exhibiting even higher reliability than regular correlation analysis on paired datasets

Since different autoencoders are independently parameterized and trained on separate data, the cell 27 embeddings learned for different omics layers could have inconsistent semantic meanings unless 28 they are linked properly. To link the autoencoders, we propose a guidance graph ?? = (??, ℰ), which incorporates prior 2 knowledge about the regulatory interactions among features at distinct omics layers,

We treat the guidance graph as observed variable and model it as generated by low-dimensional 14 feature latent variables (i.e., feature embeddings) ??(



Gut et al. 2021

To improve interpretability, methods have incorporated biological priors, like pathway defi- nitions, directly into the learning task. However, due to the correlated and redundant structure of pathways, it is difficult to determine an appropri- ate computational representation

t pathway module Variational Autoencoder (pmVAE). Our method utilizes pathway information by restricting the structure of our VAE to mirror gene-pathway memberships.

Its architecture is composed of a set of subnet- works, refered to as pathway modules, that learn interpretable multi-dimensional latent representa- tions by factorizing the latent space according to pathway gene sets. We directly address correla- tions between pathways by balancing a module- specific local loss and a global reconstruction loss

A natural way to identify which pathways are altered in a dataset is to correlate the learned parameters against external pathway or clinical data to explain the latent components. While this approach has proven fruitful (Dincer et al., 2018; Kompa & Coker, 2020; Tan et al., 2014; Way & Greene, 2018; Way et al., 2020), it requires careful analysis to iden- tify what each component is capturing, especially since all features are likely not fully disentangled (Locatello et al., 2019).

shortcomings with pathways:

- they are highly correlated and overlapping
- Many higher-level pathways (e.g., the immune system) will contain possibly disparate signals from more specific and independent pathways (e.g., T-cell and B-cell signaling) and require a richer representation.

constructs a pathway factorized latent space that directly addresses the problem of overlapping pathway definition

The pathway modules within pmVAE construct a latent space factorized by pathways. 

Given a set of K pathways, each represented as a set of genes, pmVAE consists of K pathway modules, which each behave as a VAE constrained to the set of genes that participate in its pathway. The outputs of these modules are then combined to reconstruct the expression vector of a single cell.



. pmVAE is a variational autoencoder for expression data that constructs an interpretable latent space factorized by pathway gene sets. These pathway modules encode and decode the genes contained in their gene sets, forming a latent space for each path- way. A global reconstruction is achieved by summing over all pathway module outputs and a custom training procedure is imple- mented to address optimization challenges caused by overlapping pathways.

pmVAE attempts to explain the data with the most concise set of relevant modules to explain the data. 

pmVAE’s ability to remove redundant pathway signals,

pmVAE does not find the upstream pathway scores as discriminative because the directly targeted pathways al- ready explain the variability across the cells.

pmVAE identifies non- redundant pathway representations by explaining the pertur- bation effect with only the most specific pathways.

pmVAE also finds three additional pathways to be discrimi- native of the cell’s perturbation state that is neither up- nor downstream of the targeted pathways. We believe that these biological processes are also discriminative because cell- type and perturbation status are entangled signals within our dataset.

In contrast to interpretable AE and f-scLVM, pmVAE provides a per-pathway multidi- mensional representation. This representation enables us to interrogate pathway-specific biological features, such as cell-type-specific effects

By incorporating pathway membership into the architectural design, pmVAE constructs a latent space fac- torized by pathway. This design enables direct association between the resulting pathway scores and clinically relevant features.



Oh et al. 2021

PathCNN, that constructs an interpretable CNN model on integrated multi-omics data using a newly defined pathwayimage.



To increase the in- terpretability of classical CNNs in image problems, an approach called Class Activation Mapping (CAM) was developed using global average pooling (Zhou et al., 2016). CAM produces a localization map for a target class, visualizing the discriminative image regions used by the CNN to predict the class.

CAM has some lim- itations: (i) to employ CAM, the CNN architecture used in modeling should be modified by removing fully connected layers and adding a global average pooling layer after the last convolutional layer where class activation maps are generated, and (ii) the modified network for CAM should be fine-tuned. A

A more generalizable approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), was subsequently proposed, which uses the gradient information of any target class flowing into the last convolutional layer to produce class activation maps (Selvaraju et al., 2017). 

we propose a novel method, called PathCNN,to
build an interpretable CNN model of cancer outcomes using multi- omics data. As input data to the CNN model, pseudo images of biological pathways (called pathway images) are used, which are generated in the low dimensional space of integrated multi-omics data including mRNA expression, copy number variation (CNV) and DNA methylation. After modeling, to identify the biological pathways associated with outcomes, Grad-CAM is used, for which attention maps superimposed on pathway images are analyzed to pinpoint key pathways.

the resulting CNN model achieves much better pre- dictive performance compared to other methods. Furthermore, the model employing Grad-CAM is interpretable, enabling the visual identification of influential biological pathways

Each type of omics data at a gene level was converted into pathway level profiles. To do this, pathway information was first extracted, along with the associated genes for each pathway from the Kyoto Encyclopedia of Genes and Genomes (KEGG) database (Kanehisa

s, the matrix B consists of samples in rows and genes for a given pathway in col- umns.. Using principal component analysis (PCA), the matrix B was decomposed into uncorrelated components, 

This task for the pathway pi was also carried out on the CNV matrix (C) and DNA methylation matrix (M),

The process was repeated for all 146 pathways, resulting in the merged matrices

A combined matrix of the three matrices is called the pathway image of the
sample sj, where rows represent 146 pathways, and columns repre- sent 3?q PCs combined for the three omics types, which was input into a CNN model.

, a few PCs (q=1–5) were used; for example, a sample (pathway image) is represented by a matrix with 146x6 elements for q=2, where the first and second columns are from mRNA expression, the third and fourth columns are from CNV, and the fifth and sixth columns are from DNA methylation, representing the first two PCs of each omics type.

To identify the important pathways associated with LTS in GBM, Grad-CAM was used, which is a technique to localize discriminative image regions.

Thus, if correlated pathways are clustered on path- way images, Grad-CAM is more likely to identify key pathways. This is also consistent with the nature of CNNs that capture local patterns in input images through various filters. It

It is intuitively rea- sonable to place correlated pathways in proximity on the pathway images to better localize important regions. Hence, the 146 path- ways were ordered in the following manner: Pearson correlation be- tween pathways was computed on a matrix of 146 by (number of samples ? number of PCs ? 3), generated by combining all resultant pathway images. The two most correlated pathways were placed on the top two rows on the pathway images, and then a pathway that was the most correlated with the pathway in the second row was placed on the third row. This process was repeated for all 146 path- ways. Note that the ith pathway was located among those not previ- ously selected, and it was the most correlated with the pathway in the row i?1. As a result, all pathway images had the same order of pathways.

Grad-CAM for class-discriminative localization mapping was employed to identify the important pixels (pathways) on pathway images associated with LTS in GBM patients (Fig. 3A)(Selvaraju et al., 2017). Class activation maps were generated by computing the gradient of a score (yc) for each class c with respect to feature maps A of the last convolutional layer. More specifically, neuron importance weights wc for a class c were computed

Unlike the modeling with cross validation, for biological inter-
pretation using Grad-CAM, a CNN model was built using all sam- ples. Each sample was fed into the model, leading to two activation maps (for LTS and non-LTS). A difference map between the two ac- tivation maps was thereby produced. After repeating this process for all samples, a statistical analysis was conducted.



Ma and Zhang 2019

e Factor Graph Neural Network model that is interpretable and predictable by combining probabilistic graphical models with deep learning. We directly encode biological knowledge such as Gene Ontology as a factor graph into the model architecture, making the model transparent and interpretable. Furthermore, we devised an attention mechanism that can capture multi-scale hierarchical interac- tions among biological entities such as genes and Gene Ontology terms.

We applied our model to two cancer genomic datasets to predict target clinical variables 

 Our model can also be used for gene set enrichment analysis and selecting Gene Ontology terms that are important to target clinical variables

the Factor Graph Neural Network (FGNN) model, which directly encodes biological knowledge such as Gene Ontology into the model architecture

Unlike the hidden nodes in conventional deep learning models which do not have a physical meaning, each node (i.e., “neuron”) in the Factor Graph Neural Network model corresponds to some biological entity (such as genes or Gene Ontology terms), making the model transparent and interpretable. We

We not only address the model interpretability challenge but also make the model generalize well by directly incorporating biological knowledge as inductive biases Battaglia et al. (2018) into the model architecture.

We also devised a parameter sharing mechanism to significantly reduce the number of model parameters and yet maintain the high representation power of deep learning models. 

Furthermore, we applied the attention mechanism to capture hierarchical multi-scale interactions between Gene Ontology terms and genes.

Our model can be used for gene set enrichment analysis as well. E

our proposed Factor Graph Neural Network model uses graphs from domain knowledge as model architecture and predicts clinical target variables.

Our proposed Factor Graph Neural Network model is highly expressive and interpretable by combining the strength of interpretability of factor graphs in probabilistic graphical models and the supreme representation power of deep neural network

With parameter sharing mechanism, our proposed Factor Graph Neural Network model can also be trained with stochastic depth, which can further boost the model generalizability

One key to the success of CNN and other deep learning models is the use of proper relational inductive biases (Battaglia et al., 2018). For CNN, the inductive biases include the locality (parameter sharing) and the translational invariance

To build predictable and generalizable deep learning models in the biomedical domain, we can incorporate some prior knowledge as inductive bias into the model, too. In this paper, our Factor Graph Neural Network model encodes the factor graph from domain knowledge as an inductive bias into the model architecture. It generalizes the Graph Convolutional Network (GCN) (Kipf &Welling, 2016) and can capture hierarchical multi-scale interactions with attention mechanisms.

In many applications, there are two types of variables: observable variables and latent variables. Latent variables can be seen as “factors” that are related to observable variables. For example, gene expressions are measured in many genetic disease studies. These gene expressions are observable variables. A pathway or Gene Ontology (GO) term involves multiple genes and gene products, but their activities are not directly observable. These pathways and GO terms constitute various factors in a gene network. Since these hidden factors are not directly observable, many models directly use observable variables for predicting target variables such as clinical outcomes.

to make our Factor Graph Neural Network model predictable and generalizable, we incor- porate prior knowledge such as Gene Ontology annotations as the inductive bias into the model architecture.

Genes and Gene Ontology (GO) terms form a bipartite graph. There are two types of nodes (i.e., gene nodes and GO nodes). Each GO term is associated with a number of genes and gene products. GO terms are treated as factors in the Factor Graph Neural Network model. (Note we can also use pathways or other gene sets derived from biological knowledgebase as factors.) Based on Gene Ontology annotations, we can build a factor graph with GO terms as factors and genes as observable variables. This factor graph encodes domain knowledge and can be used as an inductive bias for constructing the Factor Graph Neural Network model.



The nodes in the input layer are genes, and the nodes in the hidden layer are GO terms. The output layer is the target clinical outcomes.

There is an edge between a gene and a GO term if and only if the gene is included in the GO term. Therefore the network is not fully connected between the input layer and the hidden layer. Instead, the connections are determined by the relationships between GO terms (factors) and genes (observable variables).

The two-layer Factor Graph Neural Network model as shown in Fig. 2 is a two-layer neural network with its hidden layer corresponding to Gene Ontology terms and the edges determined by the Gene Ontology annotations. It is usually not sufficient for modeling complex nonlinear transformations with a “shallow” architecture. In order to make this model more expressive to model complex nonlinear transformations, we can unroll this Factor Graph Neural Network model to make it have more layers as deep neural networks can have more expressive power

The input layer is the observable gene expressions x. The first hidden layer corresponds to the GO terms. The second hidden layer corresponds to some latent state of genes. The rest of the hidden layers all correspond to latent states of genes or GO terms. The output layer is the target clinical variable y

By unrolling the factor graph neural network model, we introduced multiple composable nonlinear transformations, making the model able to approximate any complex nonlinear functions

, there can be infinitely many layers by unrolling the model

In order to reduce the risk of overfitting, we adopt a parameter sharing mechanism similar to recurrent neural networks and the Transformer model (Vaswani et al., 2017). Different from convolutional neural networks where parameter sharing is within a single layer, our factor graph neural network model shares parameters across layers

There are two set of parameters in this unrolled factor graph neural network model: one used to map genes to GO terms, and the other map GO terms to genes. We

In an unrolled factor graph neural network model with multiple layers, we can share these two set of parameters (corresponding to the two set of transformations from variables to factors and vice versa) across layers. As the same set of parameters are used between variables and factors in all the layers, we can significantly reduce the number of model parameters. Since there can be multiple layers corresponding to genes and GO terms, we can add skip connections as in ResNet (He et al., 2016) to connect the previous gene/GO layers to the current gene/GO layers. This can help gradient flow and speed up training.

Up to now we only considered immediate interactions among nodes in a factor graph, i.e., the direct connection between observable variables and factors. With unrolled factor graph neural network, we can naturally incorporate multi-scale hierarchical interactions into the model using attention mechanisms.

To enable multi-scale hierarchical interaction in an unrolled factor graph neural network model, we connect the lth layer with all previous (l − 1) layers

There will be an edge between a node in the lth layer and a node in the (l − 1)th layer if and only if node i can reach node j in the factor graph in one step. In other words, j is the direct neighbor of i. Similarly, there will be an edge between node i in the lth layer and node j in the
(l − k)th layer if and only if node i can reach node j in k steps. For

This idea is very much like convolutional neural networks (CNN) on graphs, where the neurons in the high-level layers will have a larger reception field. The difference is that in a CNN model, the number of neurons in each layer will be decreased by a factor of the stride, and the new feature plane no longer has a clear interpretable physical meaning. By contrast, every neuron in the factor graph neural network model corresponds to some physical entity (genes or GO terms), making the model transparent and interpretable. 

the attention matrices that are used to connect all the layers in a factor graph neural network model to capture multi-scale hierarchical interactions.
we apply the attention mechanism to assign weights to connections between different layers. 

The unrolled factor graph neural network model is based on the factor graph which encodes biological knowledge such as Gene Ontology annotations. (Note a factor graph is a data structure encoding biomedical domain knowledge, while the factor graph neural network model is a deep neural network model which uses the domain knowledge factor graph as the model backbone.

A factor graph can be seen as an undirected bipartite graph. There are two node sets: source variables and target factors. Based on the network topology, we can calculate the state transition matrices from source to target and vice versa.

calculate the attention matrices from the state transition matrices.

These attention matrices provide with the weights for the connections across different layers, which capture multi-scale hierarchical interactions among nodes.

If we have additional topological information about the factors such as Gene Ontology hierarchical structure, we can also encode the network hierarchy in the Factor Graph Neural Network model. A

This hierarchical network can be seen as a directed acyclic graph. Each node can be seen as a factor with its domain being all its children. Thus the factor graph neural network can be applied to encode network hierarchy as well.

The factor graph neural network model can also be used for gene set enrichment analysis (GSEA). Currently,

The factor graph neural network can be used to identify gene sets that are relevant to target clinical variables of interest based on stochastic gradient descent.

each GO term is associated with a set of genes and can thus be seen as a gene set. Our approach can be applied to identify enriched gene sets by training the model end-to-end. The weights of the last layer from the gene sets to the target clinical variable can be used to select gene sets that are most relevant to the target variable.



Elmarakeby et al. 2021

 P-NET—a biologically informed deep learning model—
to stratify patients with prostate cancer by treatment-resistance state and evaluate molecular drivers of treatment resistance for therapeutic targeting through complete model interpretability.



P-NET can predict cancer state using molecular data with a performance that is superior to other modelling approaches



, the biological interpretability within P-NET revealed established and novel molecularly altered candidates, such as MDM4 and FGFR1, which were implicated in predicting advanced disease and validated in vitro.

biologically informed fully interpretable neural networks enable preclinical discovery and clinical prediction in prostate cancer and may have general applicability across cancer types.

development of multiple attribution methods, including LIME20
, Deep- LIFT13
, DeepExplain21
and SHAP22 , that can be used to enhance the deep
learning explainability and understand how the model is processing information and making decisions.

customized neural network architectures that are inspired by biological systems. For example, vis- ible neural networks were developed to model the effect of gene inter- action on cell growth in yeast (DCell) and cancer cell line interactions with therapies (DrugCell)3,5
. A pathway-associated sparse deep neural
network (PASNet) used a flattened version of pathways to predict patient prognosis in Glioblastoma multiforme2

P-NET is a neural network architecture that encodes different biological entities into a neural network language with customized connections between consecutive layers (that is, features from patient profile, genes, pathways, biological processes and outcome). In this study, we focus primarily on processing mutations and copy-number alterations. The trained P-NET provides a relative ranking of nodes in each layer to inform generation of biological hypotheses.

A set of 3,007 curated biological pathways were used to build a pathway-aware multi-layered hierarchical network (P-NET) (

, the molecular profile of the individual is fed into the model and distributed over a layer of nodes representing a set of genes using weighted links

. Later layers of the network encode a set of pathways with increasing levels of abstraction, whereby lower layers represent fine pathways and later layers represent more complex biological pathways and biological processes.

The connections between different layers are constrained to follow known child–parent relationships among encoded features, genes and pathways, and as a result the network is geared toward **interpretability by design**

To evaluate the relative importance of specific genes contributing to
the model prediction, we inspected the genes layer and used the Deep- LIFT attribution method to obtain the total importance score of genes

. To understand the behaviour of trained P-NET, we checked the activation of each node in the network, where activation here represents the signed outcome of a certain node given its inputs, and tested whether this activation changed with the change of the input sample class (primary versus metastatic) (Methods).

We observed that the difference in the node activation was higher in higher layers and more concentrated in highly ranked nodes in each layer (Extended Data Fig. 8). For example, the activation distribution of the nodes of layer H3 was different when P-NET was given a primary sample compared with a resistant sample (Extended Data Fig. 8c). Thus, the interpretable architecture of P-NET can be interrogated to understand how the input information is trans- formed through layers and nodes, enabling further understanding of the state and importance of the involved biological entities.

(see fig. 3 nice vizualization with flow chart Sankey diagram)

Through evaluation of multiple layers in the P-NET trained model,
we observed convergence in TP53-associated biology contributing to CRPC. Tracing the relevance of TP53-related pathways to the gene levels, roles for TP53 and MDM2 have been previously established in prostate cancer disease progression32,34–40
, we also observed alterations
in MDM4 that contributed substantially to this network convergence.

Visualization of inner layers of P-NET shows the estimated relative importance of different nodes in each layer.

The contribution of a certain data type to the importance of each gene is depicted using the Sankey diagram—for example, the importance of the AR gene is driven mainly by gene amplification, the importance of TP53 is driven by mutation, and the importance of PTEN is driven by deletion.

P-NET provided a simple way for integrating multiple molecular features (for example, mutations, copy number variations and fusions, among others) weighted differently to reflect their impor- tance in predicting the final outcome, which previously required dif- ferent statistical approaches for each feature to enable cancer gene discovery44,45
., P-NET provided a framework for encoding
hierarchical prior knowledge using neural network languages and turn- ing these hierarchies into a computational model that can be used both for prediction and for biological discovery in clinicogenomic contexts

. Visualization of the architecture of P-NET enabled a multilevel view of the involved biological pathways and pro- cesses, which may guide researchers to develop hypotheses regarding the underlying biological processes involved in cancer progression and translate these discoveries into therapeutic opportunities.

In addition, P-NET encodes biological pathways inside the network in a hardcoded way, which makes the model dependent on the quality of the annotations used to build the model. Use of models that leverage other hardcoded biological priors (such as KEGG and Gene Ontology) or user-specified specific biological modules may further guide model development and functional evaluation

P-NET is a feedforward neural network with constraints on the nodes and edges. In P-NET, each node encodes some biological entity (for example, genes and pathways) and each edge represents a known relationship between the corresponding entities. The constraints on the nodes allow for better understanding of the state of different bio- logical components. The constraints on the edges allow us to use a large number of nodes without increasing the number of edges, which leads to a smaller number of parameters compared to fully connected networks with the same number of nodes, and thus potentially fewer computations. 

The architecture was built using the Reactome pathway datasets46

The whole Reactome dataset was downloaded and processed
to form a layered network of five layers of pathways, one layer of genes, and one layer for features.

the P-NET model is not bound to a certain architecture, as the model architecture is automatically built by reading model specifications provided by the user via a gene matrix transposed file format (.gmt) file, and custom pathways, gene sets and modules with custom hierarchies can be provided by the user

The input layer is meant to represent features that can be measured and fed into the network. The second layer represents a set of genes of interest. The higher layers represent a hierarchy of pathways and biological processes that are manually curated. The first layer of P-NET is con- nected to the next layer via a set of one-to-one connections, and each node in the next layer is connected to exactly three nodes of the input layer representing mutations, copy number amplification and copy number deletions. 

. The second layer is restricted to have connections reflecting the gene-pathway relationships as curated by the Reactome pathway dataset. The connections are encoded by a mask matrix M that is multiplied by the weights matrix W to zero-out all the con- nections that do not exist in the Reactome pathway dataset. 

For the next layers, a similar scheme is devised to control the connection between consecutive layers to reflect the real parent–child relation- ships that exist in the Reactome dataset. 

. We checked different gradient-based attribution meth- ods to rank the features in all the layers, and we chose to use the Deep- LIFT scheme as implemented in the DeepExplain library13
.

DeepLIFT is a backpropagation-based attribution approach for
assigning a sample-level importance score for each feature. In this work, we are interested in assigning scores for each node in each layer. Given a certain sample, a specific target t, and a set of layer nodes [...], DeepLIFT calculates an importance score Ci
ls, for each node on the
basis of the difference in the target activation t – t0
such that the dif-
ference equals the aggregation of the calculated scores for all the nodes.

this is an absolute score (always positive) that measures the impact of a certain node on the outcome. The activation of the cor- responding node i, however, could be positive or negative.

To reduce the bias introduced by over-annotation of certain nodes
(nodes that are member of too many pathways), we adjusted the Deep- LIFT scores using a graph informed function f that considers the con- nectivity of each node.

The implementation of the proposed system along with the reproducible results are available on GitHub (https://github.com/marakeby/pnet_prostate_paper)