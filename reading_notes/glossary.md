## global surrogate

interpretable model trained to approximate the predictions of a black box model

- We can draw conclusions about the black box model by interpreting the surrogate model.

*(https://christophm.github.io/interpretable-ml-book/)*

## intrinsic interpretability

machine learning models that are considered interpretable due to their simple structure, such as short decision trees or sparse linear models

*(https://christophm.github.io/interpretable-ml-book/)*



## local surrogate 

interpretable models that are used to  explain individual predictions of black box machine learning models (e.g. Local interpretable model-agnostic explanations - LIME)

- <a href="LIME_def_molnar2021.md">summary</a> of the explanation from Molnar 2021

*(https://christophm.github.io/interpretable-ml-book/)*

## post-hoc interpretability

application of interpretation methods after model training (e.g. permutation feature importance); can also be applied to intrinsically interpretable models

*(https://christophm.github.io/interpretable-ml-book/)*

## model-agnostic

separated from the machine learning model (e.g. model-agnostic interpretation methods) 

*(https://christophm.github.io/interpretable-ml-book/)*



